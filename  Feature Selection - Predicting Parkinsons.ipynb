{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION & CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 05/18/21\n",
    "- onl01-dtsc-ft-022221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To discuss the 3 general types of feature selection methods and give examples of each. \n",
    "- To discuss the ideal use of GridSearch/cross-validation in our modeling process. \n",
    "- To learn how to save and load models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources/References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Udemy Course: Feature Selection for Machine Learning Models](https://www.udemy.com/course/feature-selection-for-machine-learning/) - inspired much of today's content. \n",
    "- [Tamjid's Blog Post: \"Beginners guide for feature selection\"](https://tamjida.medium.com/beginners-guide-for-feature-selection-by-a-beginner-cd2158c5c36a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Predicting Parkinon's Disease from Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parkinson's Disease is a neurological disorder that affects coordination, balance, walking, and can also affect speech.\n",
    "    - [NIA - Parkinson's Disease]( https://www.nia.nih.gov/health/parkinsons-disease#:~:text=Parkinson's%20disease%20is%20a%20brain,have%20difficulty%20walking%20and%20talking)\n",
    "    \n",
    "    -[Parkinson's Foundation](https://www.parkinson.org/Understanding-Parkinsons/Symptoms/Non-Movement-Symptoms/Speech-and-Swallowing-Problems)\n",
    "    \n",
    "- This dataset was created during the publication for > \"A comparative analysis of speech signal processing algorithms for Parkinson’s disease classification and the use of the tunable Q-factor wavelet transform\" \n",
    "    - https://doi.org/10.1016/j.asoc.2018.10.022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dataset was downloaded from https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification. \n",
    ">- \"Abstract: The data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 (65.1Â±10.9).\n",
    "    - Data Source: \n",
    "\n",
    "- [Related paper](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305799?via%3Dihub)\n",
    "    - PDF located inside `reference` folder.\n",
    "    - See Table 1 on page 9.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:06.645578Z",
     "start_time": "2021-05-18T15:12:06.608818Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "## Preprocessing tools\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict,cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from sklearn import metrics\n",
    "\n",
    "## Models & Utils\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:06.683185Z",
     "start_time": "2021-05-18T15:12:06.647673Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Changing Pandas Options to see full columns in previews and info\n",
    "n=800\n",
    "pd.set_option('display.max_columns',n)\n",
    "pd.set_option(\"display.max_info_rows\", n)\n",
    "pd.set_option('display.max_info_columns',n)\n",
    "pd.set_option('display.float_format',lambda x: f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:06.721190Z",
     "start_time": "2021-05-18T15:12:06.685706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modeling Functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import project_functions as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:07.231322Z",
     "start_time": "2021-05-18T15:12:06.724225Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pd_speech_features.csv',skiprows=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:07.273740Z",
     "start_time": "2021-05-18T15:12:07.233376Z"
    }
   },
   "outputs": [],
   "source": [
    "## null value check\n",
    "nulls= df.isna().sum()\n",
    "nulls.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:07.360628Z",
     "start_time": "2021-05-18T15:12:07.276083Z"
    }
   },
   "outputs": [],
   "source": [
    "## Preview columns and dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Too many features to visualize at once. Working on a workflow in the appendix to visualzie related columns, but still work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:11.826670Z",
     "start_time": "2021-05-18T15:12:07.362122Z"
    }
   },
   "outputs": [],
   "source": [
    "corr = df.drop('id',axis=1).corr()\n",
    "print(corr.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(corr,cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to preprocess this dataset, I should identify related features based on their names and create a dictionary to be able to slice out all related columns for EDA. [Appendix'd for now]\n",
    "\n",
    "\n",
    "- Features include results of vairous speech signal processing algorithms including (see Table 1 below):\n",
    "    - Time Frequency Features\n",
    "    - Mel Frequency Cepstral Coefficients (MFCCs)\n",
    "    - Wavelet Transform based Features, \n",
    "    - Vocal Fold Features \n",
    "    - and TWQT features \n",
    "\n",
    "- Remaining Feature Questions\n",
    "\n",
    "    - [ ] Which cols are \"Fundamenal frequency parameters\"?\n",
    "    \n",
    "<img src=\"./reference/table_1.png\" width=60%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.015322Z",
     "start_time": "2021-05-18T15:12:11.829610Z"
    }
   },
   "outputs": [],
   "source": [
    "## Seeing which columns may be categorical\n",
    "df.nunique()[(df.nunique() < 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.049320Z",
     "start_time": "2021-05-18T15:12:12.017924Z"
    }
   },
   "outputs": [],
   "source": [
    "## making gender a str so its caught by pipeline\n",
    "df['gender'] = df['gender'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.091283Z",
     "start_time": "2021-05-18T15:12:12.051133Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying root names of types of features to loop through and filter out from df\n",
    "target_col = 'class'\n",
    "drop_cols = ['id']\n",
    "\n",
    "y = df[target_col].copy()\n",
    "X = df.drop(columns=[target_col,*drop_cols]).copy()\n",
    "y.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.435330Z",
     "start_time": "2021-05-18T15:12:12.093219Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.464511Z",
     "start_time": "2021-05-18T15:12:12.436950Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.536097Z",
     "start_time": "2021-05-18T15:12:12.466078Z"
    }
   },
   "outputs": [],
   "source": [
    "## saving list of numeric vs categorical feature\n",
    "num_cols = list(X_train.select_dtypes('number').columns)\n",
    "cat_cols = list(X_train.select_dtypes('object').columns)\n",
    "\n",
    "## create pipelines and column transformer\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    ('scale',MinMaxScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='constant',fill_value='MISSING')),\n",
    "    ('encoder',OneHotEncoder(sparse=False,drop='first'))])\n",
    "\n",
    "print('# of num_cols:',len(num_cols))\n",
    "print('# of cat_cols:',len(cat_cols))\n",
    "\n",
    "## COMBINE BOTH PIPELINES INTO ONE WITH COLUMN TRANSFORMER\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('num',num_transformer,num_cols),\n",
    "    ('cat',cat_transformer,cat_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.931397Z",
     "start_time": "2021-05-18T15:12:12.537919Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fit preprocessing pipeline on training data and pull out the feature names and X_cols\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Use the encoder's .get_feature_names\n",
    "cat_features = list(preprocessor.named_transformers_['cat'].named_steps['encoder']\\\n",
    "                            .get_feature_names(cat_cols))\n",
    "X_cols = num_cols+cat_features\n",
    "\n",
    "## Transform X_traian,X_test and remake dfs\n",
    "X_train_df = pd.DataFrame(preprocessor.transform(X_train),\n",
    "                          index=X_train.index, columns=X_cols)\n",
    "X_test_df = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                          index=X_test.index, columns=X_cols)\n",
    "\n",
    "## Tranform X_train and X_test and make into DataFrames\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:12.971883Z",
     "start_time": "2021-05-18T15:12:12.933534Z"
    }
   },
   "outputs": [],
   "source": [
    "y.value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling with SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:13.011441Z",
     "start_time": "2021-05-18T15:12:12.973891Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:13.047006Z",
     "start_time": "2021-05-18T15:12:13.013589Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save list of trues and falses for each cols\n",
    "smote_feats = [False]*len(num_cols) +[True]*len(cat_features)\n",
    "# smote_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:13.303563Z",
     "start_time": "2021-05-18T15:12:13.048885Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## resample training data\n",
    "smote = SMOTENC(smote_feats)\n",
    "X_train_sm,y_train_sm = smote.fit_resample(X_train_df,y_train)\n",
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting `train_test_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:13.335708Z",
     "start_time": "2021-05-18T15:12:13.305084Z"
    }
   },
   "outputs": [],
   "source": [
    "### SAVING XY DATA TO LIST TO UNPACK\n",
    "train_test_list = [X_train_sm,y_train_sm,X_test_df,y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.460104Z",
     "start_time": "2021-05-18T15:12:13.337483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline model is a lienar SVC \n",
    "svc_linear = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(svc_linear,*train_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **⭐️Feature Selection Study Group⭐️**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Office Hours for 022221FT\n",
    "- 05/18/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types  of Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filter Methods.\n",
    "- Wrapper Methods.\n",
    "- Embedded Methods.\n",
    "- Hybrid Methods (not discussed here, see resources at top of notebook for details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Filter methods: rely on the characteristics of the features themselves. Does not involve machine learning models. Ideal for quick screen and removal of irrelevant features.\n",
    "\n",
    "- Advantages:\n",
    "    - Model agnostic\n",
    "    - Less computationally expensive than other methods. \n",
    "  \n",
    "    \n",
    "- Disadvantages:\n",
    "    - Lower improvement in model performance vs other methods. \n",
    "\n",
    "\n",
    "- Example Filter Methods:\n",
    "    - Variance\n",
    "    - Correlation\n",
    "    - Univariate selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wrapper methods use predictive machine learning models to score various subsets of features. Train a new model for each feature subset.\n",
    "\n",
    "- Advantages:\n",
    "    -  Provides the best performing subset for given model type.\n",
    "    \n",
    "- Disadvantages:\n",
    "    -  Very computationally expensive\n",
    "    - May not produce best feature combos for different methods.\n",
    "    \n",
    "- Example Wrapper Methods:\n",
    "    - Forward selection\n",
    "    - Backward elimination\n",
    "    - Exhaustive Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Embedded methods performs feature selection as part of the modeling/training process.\n",
    "\n",
    "- Advantages:\n",
    "    -  Consider the interactions between features and models.\n",
    "    - Less computationally expensive than Wrapper methods (only fit the model 1 time vs many)\n",
    "    \n",
    "- Disadvantages:\n",
    "    - only available in some models.\n",
    "    - selected features may not always be appropriate for different model types\n",
    "    \n",
    "- Example Embedded Methods:\n",
    "    - Lasso Regression\n",
    "    - Tree importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Methods - Applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overall Filter Methods Process:\n",
    "    1. Rank each feature according to some criterion\n",
    "    2. Select features with highest ranking. \n",
    "- Example Filter Methods (used below):\n",
    "    1. Variance Threshold\n",
    "    2. Correlation\n",
    "    3. Mutual Information\n",
    "    4. Univariate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.492058Z",
     "start_time": "2021-05-18T15:12:14.461779Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM1: Finding Constant & Quasi-Constant Features with `VarianceThreshold`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `VarianceThreshold`:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constant Features have the same value for every observation.\n",
    "- Quasi-Constant Features have 95-98% of the same value for one feature. \n",
    "\n",
    "- Using sklearn's VarianceThreshold with either `threshold=0.0` for constant features or `threshold=0.01` for quasi-constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.525607Z",
     "start_time": "2021-05-18T15:12:14.493820Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.578629Z",
     "start_time": "2021-05-18T15:12:14.531600Z"
    }
   },
   "outputs": [],
   "source": [
    "## checking for constant-features\n",
    "selector = VarianceThreshold(threshold=0.00)\n",
    "selector.fit(X_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.616293Z",
     "start_time": "2021-05-18T15:12:14.581243Z"
    }
   },
   "outputs": [],
   "source": [
    "## get support returns true/false for keeping features\n",
    "keep_features = selector.get_support()\n",
    "print(keep_features.sum())\n",
    "keep_features.sum()==len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No constant-features found in dataset. Check for quasi-constant (threshold=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.662962Z",
     "start_time": "2021-05-18T15:12:14.618185Z"
    }
   },
   "outputs": [],
   "source": [
    "## checking for constant-features\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "selector.fit(X_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.697530Z",
     "start_time": "2021-05-18T15:12:14.664858Z"
    }
   },
   "outputs": [],
   "source": [
    "## get support returns true/false for keeping features\n",
    "keep_features = selector.get_support()\n",
    "print(keep_features.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.732824Z",
     "start_time": "2021-05-18T15:12:14.699338Z"
    }
   },
   "outputs": [],
   "source": [
    "keep_features.shape, X_train_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:14.933587Z",
     "start_time": "2021-05-18T15:12:14.734575Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_sel = X_train_sm.loc[:,keep_features]\n",
    "X_test_sel = X_test_df.loc[:,keep_features]\n",
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:15.710420Z",
     "start_time": "2021-05-18T15:12:14.935504Z"
    }
   },
   "outputs": [],
   "source": [
    "# tic = time() #timing!\n",
    "svc_linear = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(svc_linear,X_train_sel,y_train_sm,X_test_sel,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:15.746271Z",
     "start_time": "2021-05-18T15:12:15.712292Z"
    }
   },
   "outputs": [],
   "source": [
    "## save to dict\n",
    "selected_features['variance'] = keep_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM2: Using Correlation to identify & remove highly-correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:15.785382Z",
     "start_time": "2021-05-18T15:12:15.748241Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_list_of_corrs(df,drop=[],\n",
    "                      cutoff=0.75,only_above_cutoff=False,\n",
    "                     sort_by_col=False):\n",
    "    \"\"\"Get dataframe of correlated features, with the option to only show the\n",
    "    features with correlations > cutoff\"\"\"\n",
    "    ## Claculate correlation and convert to 3-column table.\n",
    "    corr_df = df.drop(drop,axis=1).corr().unstack().reset_index()\n",
    "    \n",
    "    ## Remove self-correlations\n",
    "    corr_df = corr_df.loc[ corr_df['level_0']!=corr_df['level_1']]\n",
    "    \n",
    "    ## Make one column with unique names and drop duplicate pairs of cols\n",
    "    corr_df['columns'] = corr_df.apply(\n",
    "        lambda row: '_'.join(set(row[['level_0','level_1']] )), axis=1)\n",
    "    corr_df.drop_duplicates(subset=['columns'],inplace=True)\n",
    "    \n",
    "    ## Rename Columns\n",
    "    corr_df.rename({0:'r','level_0':'Column1',\n",
    "               'level_1':'Column2'},axis=1,inplace=True)     \n",
    "\n",
    "    ## Check if above cutoff \n",
    "    corr_df['above_cutoff'] = corr_df['r'] > cutoff\n",
    " \n",
    "    ## Sort by col or by r-value\n",
    "    if sort_by_col:\n",
    "        corr_df = corr_df.sort_values( ['Column1','Column2'],ascending=True)\n",
    "    else:\n",
    "        corr_df =  corr_df.sort_values('r',ascending=False)\n",
    "        \n",
    "    \n",
    "    ## Return only those above cutoff\n",
    "    if only_above_cutoff:\n",
    "        corr_df = corr_df[corr_df['above_cutoff']==True]\n",
    "        \n",
    "    ## Reset Index for Aesthetics\n",
    "    corr_df.reset_index(drop=True)\n",
    "    return corr_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:15.819712Z",
     "start_time": "2021-05-18T15:12:15.787221Z"
    }
   },
   "outputs": [],
   "source": [
    "# corr_df = get_list_of_corrs(df,cutoff=0.75, only_above_cutoff=True)\n",
    "# corr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As with our Linear Regression, we would want to remove features that are highly multicollinear. (have correlation >0.7-0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM3: Using Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Wikipedia: Mutual Infromation](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7f3385f4d779f062696c134223b5683e754a6f1c\"> \n",
    "\n",
    "- Mutual Information represents how much we can learn about the target from our features. \n",
    "    - The higher the value for mi the more information a feature contains about the target.\n",
    "    - We want to keep features with the highest mutual information with the target.\n",
    "    \n",
    "    \n",
    "- How many features to keep is somewhat arbitrary.\n",
    "    - Can use `SelectKBest` to select top `K` m.i. features \n",
    "    - Can use `SelectPercentile` to select top %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:15.853877Z",
     "start_time": "2021-05-18T15:12:15.821532Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:19.637172Z",
     "start_time": "2021-05-18T15:12:15.856017Z"
    }
   },
   "outputs": [],
   "source": [
    "mi = mutual_info_classif(X_train_sm, y_train_sm)\n",
    "mi[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:12:19.671172Z",
     "start_time": "2021-05-18T15:12:19.638867Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a series so we can see which features\n",
    "mi_scores = pd.Series(mi,index=X_train_sm.columns).sort_values(ascending=False)\n",
    "mi_scores = mi_scores.to_frame('MI')\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Must choose to select top # or percentile of features to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:13:15.194899Z",
     "start_time": "2021-05-18T15:13:11.505364Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 200\n",
    "top_k_selector = SelectKBest(mutual_info_classif,k=k).fit(X_train_sm,y_train_sm)\n",
    "top_k_columns = X_train_sm.columns[top_k_selector.get_support()]\n",
    "top_k_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:14:01.012040Z",
     "start_time": "2021-05-18T15:14:00.824065Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sel = X_train_sm.loc[:,top_k_columns]\n",
    "X_test_sel = X_test_df.loc[:,top_k_columns]\n",
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:14:04.566499Z",
     "start_time": "2021-05-18T15:14:03.991613Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(svc_linear,X_train_sel,y_train_sm,X_test_sel,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FM4: Univariate Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While not demo'd in this notebook,an example univariate modeling approach would be to take all of the features of a house one at a time to make separate simple linear regression models. \n",
    "- Then, select the top K features that had the good performance (R-Squared)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods Applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overall Wrapper Methods Process:\n",
    "    - Use a specific classifier to select the optimal number of features. \n",
    "    - General approach is to create many recursive models where a feature is added or removed from the dataset and the performance is scored. \n",
    "    - Greedy search algorithms (will try all options)\n",
    "\n",
    "___\n",
    "\n",
    "- Example Wrapper Methods (used below):\n",
    "    1. Stepwise Forward Selection \n",
    "    2. Stepwise Backward Selection/Recursive Feature Elimination. \n",
    "    \n",
    "    3. Exhaustive Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WM1&2: Stepwise Forward/Backwards Selection with `mlxtend`'s `SequentialFeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [mlxtend Sequential Feature Selector](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:14:27.321070Z",
     "start_time": "2021-05-18T15:14:27.275259Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To use SFS, must provide:\n",
    "    1. Make an instance of the model you wish to optimize for (e.g. Random Forests, SVC)\n",
    "    2. Choose a stopping criterion (e.g. select 10 features).\n",
    "    3. Specify if want to step forward or backward. \n",
    "    4. Evaluation metric to use\n",
    "    5. Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:15:55.302780Z",
     "start_time": "2021-05-18T15:15:05.170234Z"
    }
   },
   "outputs": [],
   "source": [
    "# svc_linear = SVC(kernel='linear',C=1)\n",
    "sfs = SequentialFeatureSelector( SVC(kernel='linear',C=1), k_features=25,\n",
    "                               forward=True, floating=True,\n",
    "                                verbose=2, cv=2,\n",
    "                                n_jobs=-1)\n",
    "sfs.fit(X_train_sm,y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:15:03.065818Z",
     "start_time": "2021-05-18T15:15:03.018533Z"
    }
   },
   "outputs": [],
   "source": [
    "sfs.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:16:47.000292Z",
     "start_time": "2021-05-18T15:16:46.965201Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:17:54.522098Z",
     "start_time": "2021-05-18T15:17:54.486541Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = list(sfs.k_feature_names_)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:01.832398Z",
     "start_time": "2021-05-18T15:18:01.782567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sel = X_train_sm[selected_features]\n",
    "X_test_sel = X_test_df[selected_features]\n",
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:03.283108Z",
     "start_time": "2021-05-18T15:18:02.883125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(svc_linear,X_train_sel,y_train_sm,X_test_sel,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WM3: Exhaustive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T20:29:38.249662Z",
     "start_time": "2021-05-17T20:29:38.157168Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overall Embedded Methods Process:\n",
    "    1. Train a machine learning model (Feature selection performed during the model's training. )\n",
    "    2. Derive feature importance from the model\n",
    "    3. Remove non-important features.\n",
    "___\n",
    "\n",
    "- Example Embedded Methods (used below):\n",
    "    1. Regression Coefficients \n",
    "    2. Tree importance \n",
    "    3. LASSO/L1-Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:17.334019Z",
     "start_time": "2021-05-18T15:18:17.300299Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:24.050382Z",
     "start_time": "2021-05-18T15:18:23.589570Z"
    }
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=1e12)\n",
    "pf.fit_and_time_model(log_reg,*train_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:36.113066Z",
     "start_time": "2021-05-18T15:18:36.017485Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(log_reg).fit(X_train_sm,y_train_sm)\n",
    "selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:38.665002Z",
     "start_time": "2021-05-18T15:18:38.629537Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_features = selector.get_support()\n",
    "X_train_sm.columns[logreg_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:18:40.844609Z",
     "start_time": "2021-05-18T15:18:40.809029Z"
    }
   },
   "outputs": [],
   "source": [
    "coeffs = pd.Series(selector.estimator_.coef_.flatten(),\n",
    "                   index=X_train_sm.columns)\n",
    "coeffs[logreg_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:19:36.169817Z",
     "start_time": "2021-05-18T15:19:36.008716Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sel = X_train_sm.loc[:,logreg_features]\n",
    "X_test_sel = X_test_df.loc[:,logreg_features]\n",
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:19:38.009278Z",
     "start_time": "2021-05-18T15:19:37.368421Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(svc_linear,X_train_sel,y_train_sm,X_test_sel,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression Coefficients  With Lasso/L1 Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:20:02.470298Z",
     "start_time": "2021-05-18T15:20:01.991140Z"
    }
   },
   "outputs": [],
   "source": [
    "l1_reg = LogisticRegression(C=0.5,penalty='l1',solver='liblinear')\n",
    "pf.fit_and_time_model(l1_reg,*train_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:20:05.860337Z",
     "start_time": "2021-05-18T15:20:05.745917Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(l1_reg)\n",
    "selector.fit(X_train_sm,y_train_sm)\n",
    "lasso_features = selector.get_support()\n",
    "lasso_features.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:20:06.791783Z",
     "start_time": "2021-05-18T15:20:06.754777Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sm.columns[lasso_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:20:12.102008Z",
     "start_time": "2021-05-18T15:20:12.040352Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sel = X_train_sm.loc[:,lasso_features]\n",
    "X_test_sel = X_test_df.loc[:,lasso_features]\n",
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:20:14.104299Z",
     "start_time": "2021-05-18T15:20:13.684321Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(svc_linear,X_train_sel,y_train_sm,X_test_sel,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:21:04.216308Z",
     "start_time": "2021-05-18T15:21:04.182772Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "# pf.fit_and_time_model(rf,*train_test_list)\n",
    "# importances = pf.get_importance(rf,X_train_df)\n",
    "# importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:21:12.834254Z",
     "start_time": "2021-05-18T15:21:11.881708Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(rf).fit(X_train_sm,y_train_sm)\n",
    "rf_features = selector.get_support()\n",
    "rf_features.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:21:39.707831Z",
     "start_time": "2021-05-18T15:21:39.584324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sel = X_train_sm.loc[:,rf_features]\n",
    "X_test_sel = X_test_df.loc[:,rf_features]\n",
    "X_train_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:21:40.735765Z",
     "start_time": "2021-05-18T15:21:40.110580Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(svc_linear,X_train_sel,y_train_sm,X_test_sel,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⭐️**Cross Validation**⭐️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation Workflow: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train/Test split\n",
    "2. Create a model. \n",
    "3. Apply Cross Validation with the training data (`GridsearchCV`,`cross_validate`,`cross_val_score`,`cross_val_predict`) to asses your model/hyperparameter choices.\n",
    "\n",
    "4. Evaluate Cross validation scores\n",
    "    - If not happy with the scores:\n",
    "        - Try different model/hyperparameters.\n",
    "    - If happy with scores/performance:\n",
    "        - Train an **individual** model (not-cv) (or take gridsearch's `.best_estimator_` on the **training data** and **evaluate with the test data.**\n",
    "\n",
    "\n",
    "\n",
    "5. If individual model performs well on test data (isn't overfit) **and you are planning to deploy the model:** \n",
    "    - You would **re-train the model** on the **entire combined data set** (X =X_train+X_test, y=y_train+y_test) before pickling/saving the model.\n",
    "    \n",
    "    \n",
    ">- ***Note: step 5 is intended for deploying models and is not required.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Different Ways of Cross-Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:23:52.620485Z",
     "start_time": "2021-05-18T15:23:52.586357Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate,cross_val_score,cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation **functions** from sklearn.model_selection:\n",
    "    - `cross_validate`: \n",
    "        - returns dict of K-fold  scores for the training data, including the training times.\n",
    "    - `cross_val_score`:\n",
    "        - returns the K-fold validation scores for the K-fold's test-splits\n",
    "        \n",
    "    - `cross_val_predict`:\n",
    "        - returns predictions from the cross validated model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:30.687692Z",
     "start_time": "2021-05-18T15:27:30.474815Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_final = X_train_sm.loc[:,logreg_features]\n",
    "X_test_final = X_test_df.loc[:,logreg_features]\n",
    "display(X_train_final.head(),X_test_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:31.344080Z",
     "start_time": "2021-05-18T15:27:31.309584Z"
    }
   },
   "outputs": [],
   "source": [
    "## make an instance of a model\n",
    "clf = SVC(kernel='linear',C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:32.258900Z",
     "start_time": "2021-05-18T15:27:31.859207Z"
    }
   },
   "outputs": [],
   "source": [
    "## cross_validate  returns scores and times\n",
    "cv_results = cross_validate(clf,X_train_final,y_train_sm,scoring='recall')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:32.873652Z",
     "start_time": "2021-05-18T15:27:32.429196Z"
    }
   },
   "outputs": [],
   "source": [
    "## cross_val_score reutns scores\n",
    "cv_score = cross_val_score(clf,X_train_final,y_train_sm,scoring='recall')\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:33.427054Z",
     "start_time": "2021-05-18T15:27:32.972279Z"
    }
   },
   "outputs": [],
   "source": [
    "## cross_val_predict returns predictions that can be used to validate\n",
    "y_hat_train_cv = cross_val_predict(clf,X_train_final,y_train_sm)\n",
    "print(metrics.classification_report(y_train_sm, y_hat_train_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:34.146053Z",
     "start_time": "2021-05-18T15:27:33.530731Z"
    }
   },
   "outputs": [],
   "source": [
    "## If happy with results, train an individual model and evaluate with test data\n",
    "clf = SVC(kernel='linear',C=1)\n",
    "pf.fit_and_time_model(clf,X_train_final,y_train_sm,X_test_final, y_test,scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:35.792797Z",
     "start_time": "2021-05-18T15:27:34.741918Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## If happy with train/test split results, can re-train model on entire dataset\n",
    "X_tf = preprocessor.fit_transform(X)\n",
    "svc_linear.fit(X_tf,y)\n",
    "pf.evaluate_classification(svc_linear,X_tf,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⭐️**Saving Models**⭐️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Guide on Saving Models: \n",
    "    - https://scikit-learn.org/stable/modules/model_persistence.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `Pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:45.888284Z",
     "start_time": "2021-05-18T15:27:45.854153Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf,open('best_model.pickle','wb'))\n",
    "# s = pickle.dumps(clf)\n",
    "# type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:27:51.136779Z",
     "start_time": "2021-05-18T15:27:51.100489Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_pickle = pickle.load(open('best_model.pickle','rb'))\n",
    "loaded_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:28:01.288835Z",
     "start_time": "2021-05-18T15:28:00.774484Z"
    }
   },
   "outputs": [],
   "source": [
    "pf.evaluate_classification(loaded_pickle,X_test_final,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `joblib` (sklearn's preferred method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:28:11.523672Z",
     "start_time": "2021-05-18T15:28:11.484907Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'best_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:28:11.774998Z",
     "start_time": "2021-05-18T15:28:11.736108Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_jb = joblib.load('best_model.joblib')\n",
    "clf_jb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T15:28:19.325574Z",
     "start_time": "2021-05-18T15:28:18.926902Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pf.evaluate_classification(clf_jb,X_test_final,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many different ways to select features for your models, each with advantages & disadvantages.\n",
    "- Depending on the size of your dataset and the number of features will determine how much you need to worry about performing feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "## iNTERPRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T19:02:18.498520Z",
     "start_time": "2021-05-17T19:01:29.482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from mlxtend "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## CONCLUSIONS & RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Summarize your conclusions and bullet-point your list of recommendations, which are based on your modeling results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Grouping Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:26.664255Z",
     "start_time": "2021-05-17T18:38:26.659670Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Defining Clusters of related columns for EDA/preprocessing\n",
    "\n",
    "feature_types = dict(patient_info = ['id','gender'], \n",
    "     baseline = ['Jitter', 'Shimmer','Harmonicity', 'RPDE','DFA',\"PPE\"],\n",
    "     time_frequency = ['intensity'], \n",
    "     mel_spectrogram = ['MFCC'],\n",
    "     tqwt = ['tqwt'])\n",
    "\n",
    "feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:26.669271Z",
     "start_time": "2021-05-17T18:38:26.667590Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Quick test filter for stub names\n",
    "# list(filter(lambda x: 'intensity' in x.lower(),df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:26.675787Z",
     "start_time": "2021-05-17T18:38:26.671832Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_feature_dict(df,feature_types):\n",
    "    \"\"\"Finds column names by recognizing name stubs (partial col names)\n",
    "    \n",
    "    Args:\n",
    "        df (Frame): df with columns to filter.\n",
    "        feature_types (dict): dict with category of features as the first key\n",
    "        and a list of stub names of columns that belong to that category.\n",
    "        \n",
    "    Returns:\n",
    "        feature_cols: dict of filtered columns grouped by \"feature_types\" keys.\n",
    "        all_columns: list of all filtered columns without grouping.\n",
    "        \n",
    "        \n",
    "    EXAMPLE USAGE:\n",
    "    >>  feature_types = dict(patient_info = ['id','gender'], \n",
    "                        time_frequency = ['intensity'],\n",
    "                        baseline = ['Jitter','Harmonicity'])\n",
    "    >> feature_cols ,all_cols = make_feature_dict(df,feature_types)\n",
    "    >> feature_cols\n",
    "    ## RETURNS: \n",
    "    {'patient_info': ['id', 'gender'],\n",
    "     'time_frequency': ['minIntensity', 'maxIntensity', 'meanIntensity'],\n",
    "     'baseline': ['locPctJitter',\n",
    "      'locAbsJitter',\n",
    "      'rapJitter',\n",
    "      'ppq5Jitter',\n",
    "      'ddpJitter',\n",
    "      'meanAutoCorrHarmonicity',\n",
    "      'meanNoiseToHarmHarmonicity',\n",
    "      'meanHarmToNoiseHarmonicity']}\n",
    "        \"\"\"\n",
    "    ## create epty dict to fill in related features and empty list for all cols\n",
    "    feature_cols = {}\n",
    "    all_columns= []\n",
    "    \n",
    "    ## For each feature type and the list of stub names\n",
    "    for feat_type, name_list in feature_types.items():\n",
    "#         feature_cols[feat_type] = {}\n",
    "\n",
    "        ## Maker a list to handle single-column results \n",
    "        curr_type_cols = []\n",
    "        \n",
    "        ## For each name stub\n",
    "        for name in name_list:\n",
    "            ## Get all columns containing stub\n",
    "            cols = [c for c in df.columns if name.lower() in c.lower()]\n",
    "            \n",
    "            ## Add cols to both current type and all columns\n",
    "            curr_type_cols.extend(cols)\n",
    "            all_columns.extend(cols)\n",
    "            \n",
    "            ## save list of columns under feature_type\n",
    "            feature_cols[feat_type] = curr_type_cols\n",
    "            \n",
    "            \n",
    "            ### OLD CODE WHEN ORIGINALLY USING NESTED DICT\n",
    "#             ## If the name \n",
    "#             if name.lower() == feat_type.lower():\n",
    "#                 feature_cols[feat_type] = cols\n",
    "                \n",
    "#             else:\n",
    "#                 ## combine names\n",
    "#                 feature_cols[feat_type] = cols\n",
    "                \n",
    "            \n",
    "            \n",
    "    return feature_cols, all_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:26.681921Z",
     "start_time": "2021-05-17T18:38:26.677098Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Saving dict of all identified clusters of features\n",
    "feature_cols,filtered_cols = make_feature_dict(df,feature_types)\n",
    "feature_cols.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:26.686450Z",
     "start_time": "2021-05-17T18:38:26.683743Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## How many cols grabbed by function\n",
    "len(filtered_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:26.703452Z",
     "start_time": "2021-05-17T18:38:26.687994Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## testing feat_cols dict\n",
    "feature_cols['baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### BOOKMARK FOR LATER: Sorting out remaining cols to group/filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:26.709792Z",
     "start_time": "2021-05-17T18:38:26.705610Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_unmatched = df.drop(columns=filtered_cols)\n",
    "# df_unmatched.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:53.026937Z",
     "start_time": "2021-05-17T18:38:52.987902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier()\n",
    "# pf.fit_and_time_model(rf,*train_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-17T18:38:53.265387Z",
     "start_time": "2021-05-17T18:38:53.225938Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pf.get_importance(rf,X_test_df,top_n=100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
