{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS CHECKLIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Keep in mind that it is normal to jump between the OSEMN phases and some of them will blend together, like SCRUB and EXPLORE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **[OBTAIN](#OBTAIN)**\n",
    "    - Import data, inspect, check for datatypes to convert and null values\n",
    "    - Display header and info.\n",
    "    - Drop any unneeded columns, if known (`df.drop(['col1','col2'],axis=1,inplace=True`)\n",
    "    <br><br>\n",
    "\n",
    "\n",
    "2. **[SCRUB](#SCRUB)**\n",
    "    - Recast data types, identify outliers, check for multicollinearity, normalize data**\n",
    "    - Check and cast data types\n",
    "        - [ ] Check for #'s that are store as objects (`df.info()`,`df.describe()`)\n",
    "            - when converting to #'s, look for odd values (like many 0's), or strings that can't be converted.\n",
    "            - Decide how to deal weird/null values (`df.unique()`, `df.isna().sum()`)\n",
    "            - `df.fillna(subset=['col_with_nulls'],'fill_value')`, `df.replace()`\n",
    "        - [ ] Check for categorical variables stored as integers.\n",
    "            - May be easier to tell when you make a scatter plotm or `pd.plotting.scatter_matrix()`\n",
    "            \n",
    "    - [ ] Check for missing values  (df.isna().sum())\n",
    "        - Can drop rows or colums\n",
    "        - For missing numeric data with median or bin/convert to categorical\n",
    "        - For missing categorical data: make NaN own category OR replace with most common category\n",
    "    - [ ] Check for multicollinearity\n",
    "        - Use seaborn to make correlation matrix plot \n",
    "        - Good rule of thumb is anything over 0.75 corr is high, remove the variable that has the most correl with the largest # of variables\n",
    "    - [ ] Normalize data (may want to do after some exploring)\n",
    "        - Most popular is Z-scoring (but won't fix skew) \n",
    "        - Can log-transform to fix skewed data\n",
    "    \n",
    "    \n",
    "3. **[EXPLORE](#EXPLORE)**\n",
    "    - [ ] Check distributions, outliers, etc**\n",
    "    - [ ] Check scales, ranges (df.describe())\n",
    "    - [ ] Check histograms to get an idea of distributions (df.hist()) and data transformations to perform.\n",
    "        - Can also do kernel density estimates\n",
    "    - [ ] Use scatter plots to check for linearity and possible categorical variables (`df.plot(\"x\",\"y\")`)\n",
    "        - categoricals will look like vertical lines\n",
    "    - [ ] Use `pd.plotting.scatter_matrix(df)` to visualize possible relationships\n",
    "    - [ ] Check for linearity.\n",
    "   \n",
    "   \n",
    "4. **[MODEL](#MODEL)**\n",
    "\n",
    "    - **Fit an initial model:** \n",
    "        - Run an initial model and get results\n",
    "\n",
    "    - **Holdout validation / Train/test split**\n",
    "        - use sklearn `train_test_split`\n",
    "    \n",
    "    \n",
    "5. **[iNTERPRET](#iNTERPRET)**\n",
    "    - **Assessing the model:**\n",
    "        - Assess parameters (slope,intercept)\n",
    "        - Check if the model explains the variation in the data (RMSE, F, R_square)\n",
    "        - *Are the coeffs, slopes, intercepts in appropriate units?*\n",
    "        - *Whats the impact of collinearity? Can we ignore?*\n",
    "        <br><br>\n",
    "    - **Revise the fitted model**\n",
    "        - Multicollinearity is big issue for lin regression and cannot fully remove it\n",
    "        - Use the predictive ability of model to test it (like R2 and RMSE)\n",
    "        - Check for missed non-linearity\n",
    "        \n",
    "       \n",
    "6. **Interpret final model and draw >=3 conclusions and recommendations from dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T18:00:23.504059Z",
     "start_time": "2020-01-29T18:00:23.498461Z"
    }
   },
   "source": [
    "<div style=\"display:block;border-bottom:solid red 3px;padding:1.4em;color:red;font-size:30pt;display:inline-block;line-height:1.5em;\">\n",
    "DELETE THIS CELL AND EVERYTHING ABOVE FROM YOUR FINAL NOTEBOOK\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out:\n",
    "* Student name:  James Irving\n",
    "* Student pace: ~~self paced / part time / full time~~: all-the-time\n",
    "* Scheduled project review date/time: Now & Forever\n",
    "* Instructor name: Life/Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLE OF CONTENTS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Click to jump to matching Markdown Header.*<br><br>\n",
    "\n",
    "<font size=3rem>\n",
    "    \n",
    "- **[Introduction](#INTRODUCTION)<br>**\n",
    "- **[OBTAIN](#OBTAIN)**<br>\n",
    "- **[SCRUB](#SCRUB)**<br>\n",
    "- **[EXPLORE](#EXPLORE)**<br>\n",
    "- **[MODEL](#MODEL)**<br>\n",
    "- **[iNTERPRET](#iNTERPRET)**<br>\n",
    "- **[Conclusions/Recommendations](#CONCLUSIONS-&-RECOMMENDATIONS)<br>**\n",
    "</font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- \"Abstract: The data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 (65.1Â±10.9).\n",
    "    - Data Source: https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification\n",
    "\n",
    "- [Related paper](https://www.sciencedirect.com/science/article/abs/pii/S1568494618305799?via%3Dihub)\n",
    "    - PDF located inside `reference` folder.\n",
    "    - See Table 1 on page 9.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:17:35.213749Z",
     "start_time": "2021-05-14T21:17:35.092952Z"
    }
   },
   "outputs": [],
   "source": [
    "ls 'reference/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Feature Ranking/Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:17:36.145802Z",
     "start_time": "2021-05-14T21:17:36.136421Z"
    }
   },
   "outputs": [],
   "source": [
    ","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:18:59.261745Z",
     "start_time": "2021-05-14T21:18:58.064295Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "## Preprocessing tools\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict,cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "\n",
    "\n",
    "## Models & Utils\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:22:23.253346Z",
     "start_time": "2021-05-14T21:22:23.241842Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0e06240db380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitjs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "# Explainers\n",
    "import shap\n",
    "print(shap.__version__)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:13:19.053296Z",
     "start_time": "2021-05-14T21:13:19.047364Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Changing Pandas Options to see full columns in previews and info\n",
    "n=800\n",
    "pd.set_option('display.max_columns',n)\n",
    "pd.set_option(\"display.max_info_rows\", n)\n",
    "pd.set_option('display.max_info_columns',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:13:22.489703Z",
     "start_time": "2021-05-14T21:13:21.757215Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pd_speech_features.csv',skiprows=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.379100Z",
     "start_time": "2021-05-14T21:12:12.373836Z"
    }
   },
   "outputs": [],
   "source": [
    "## null value check\n",
    "nulls= df.isna().sum()\n",
    "# nulls[nulls>0]\n",
    "nulls.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.538914Z",
     "start_time": "2021-05-14T21:12:12.380941Z"
    }
   },
   "outputs": [],
   "source": [
    "## Preview columns and dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to preprocess this dataset, I should identify related features based on their names and create a dictionary to be able to slice out all related columns for EDA.\n",
    "\n",
    "\n",
    "- Features include results of vairous speech signal processing algorithms including (see Table 1 below):\n",
    "    - Time Frequency Features\n",
    "    - Mel Frequency Cepstral Coefficients (MFCCs)\n",
    "    - Wavelet Transform based Features, \n",
    "    - Vocal Fold Features \n",
    "    - and TWQT features \n",
    "\n",
    "- Remaining Feature Questions\n",
    "\n",
    "    - [ ] Which cols are \"Fundamenal frequency parameters\"?\n",
    "    \n",
    "<img src=\"./reference/table_1.png\" width=60%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.545566Z",
     "start_time": "2021-05-14T21:12:12.540874Z"
    }
   },
   "outputs": [],
   "source": [
    "## Defining Clusters of related columns for EDA/preprocessing\n",
    "\n",
    "feature_types = dict(patient_info = ['id','gender'], \n",
    "     baseline = ['Jitter', 'Shimmer','Harmonicity', 'RPDE','DFA',\"PPE\"],\n",
    "     time_frequency = ['intensity'], \n",
    "     mel_spectrogram = ['MFCC'],\n",
    "     tqwt = ['tqwt'])\n",
    "\n",
    "feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.550605Z",
     "start_time": "2021-05-14T21:12:12.549011Z"
    }
   },
   "outputs": [],
   "source": [
    "## Quick test filter for stub names\n",
    "# list(filter(lambda x: 'intensity' in x.lower(),df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.557815Z",
     "start_time": "2021-05-14T21:12:12.553110Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_feature_dict(df,feature_types):\n",
    "    \"\"\"Finds column names by recognizing name stubs (partial col names)\n",
    "    \n",
    "    Args:\n",
    "        df (Frame): df with columns to filter.\n",
    "        feature_types (dict): dict with category of features as the first key\n",
    "        and a list of stub names of columns that belong to that category.\n",
    "        \n",
    "    Returns:\n",
    "        feature_cols: dict of filtered columns grouped by \"feature_types\" keys.\n",
    "        all_columns: list of all filtered columns without grouping.\n",
    "        \n",
    "        \n",
    "    EXAMPLE USAGE:\n",
    "    >>  feature_types = dict(patient_info = ['id','gender'], \n",
    "                        time_frequency = ['intensity'],\n",
    "                        baseline = ['Jitter','Harmonicity'])\n",
    "    >> feature_cols ,all_cols = make_feature_dict(df,feature_types)\n",
    "    >> feature_cols\n",
    "    ## RETURNS: \n",
    "    {'patient_info': ['id', 'gender'],\n",
    "     'time_frequency': ['minIntensity', 'maxIntensity', 'meanIntensity'],\n",
    "     'baseline': ['locPctJitter',\n",
    "      'locAbsJitter',\n",
    "      'rapJitter',\n",
    "      'ppq5Jitter',\n",
    "      'ddpJitter',\n",
    "      'meanAutoCorrHarmonicity',\n",
    "      'meanNoiseToHarmHarmonicity',\n",
    "      'meanHarmToNoiseHarmonicity']}\n",
    "        \"\"\"\n",
    "    ## create epty dict to fill in related features and empty list for all cols\n",
    "    feature_cols = {}\n",
    "    all_columns= []\n",
    "    \n",
    "    ## For each feature type and the list of stub names\n",
    "    for feat_type, name_list in feature_types.items():\n",
    "#         feature_cols[feat_type] = {}\n",
    "\n",
    "        ## Maker a list to handle single-column results \n",
    "        curr_type_cols = []\n",
    "        \n",
    "        ## For each name stub\n",
    "        for name in name_list:\n",
    "            ## Get all columns containing stub\n",
    "            cols = [c for c in df.columns if name.lower() in c.lower()]\n",
    "            \n",
    "            ## Add cols to both current type and all columns\n",
    "            curr_type_cols.extend(cols)\n",
    "            all_columns.extend(cols)\n",
    "            \n",
    "            ## save list of columns under feature_type\n",
    "            feature_cols[feat_type] = curr_type_cols\n",
    "            \n",
    "            \n",
    "            ### OLD CODE WHEN ORIGINALLY USING NESTED DICT\n",
    "#             ## If the name \n",
    "#             if name.lower() == feat_type.lower():\n",
    "#                 feature_cols[feat_type] = cols\n",
    "                \n",
    "#             else:\n",
    "#                 ## combine names\n",
    "#                 feature_cols[feat_type] = cols\n",
    "                \n",
    "            \n",
    "            \n",
    "    return feature_cols, all_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.564999Z",
     "start_time": "2021-05-14T21:12:12.559720Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving dict of all identified clusters of features\n",
    "feature_cols,filtered_cols = make_feature_dict(df,feature_types)\n",
    "feature_cols.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.569602Z",
     "start_time": "2021-05-14T21:12:12.566591Z"
    }
   },
   "outputs": [],
   "source": [
    "len(filtered_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.574020Z",
     "start_time": "2021-05-14T21:12:12.571115Z"
    }
   },
   "outputs": [],
   "source": [
    "## testing feat_cols dict\n",
    "feature_cols['baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOOKMARK FOR LATER: Sorting out remaining cols to group/filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.577094Z",
     "start_time": "2021-05-14T21:12:12.575476Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_unmatched = df.drop(columns=filtered_cols)\n",
    "# df_unmatched.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.751722Z",
     "start_time": "2021-05-14T21:12:12.578377Z"
    }
   },
   "outputs": [],
   "source": [
    "## Seeing which columns may be categorical\n",
    "df.nunique()[(df.nunique() < 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.759955Z",
     "start_time": "2021-05-14T21:12:12.754397Z"
    }
   },
   "outputs": [],
   "source": [
    "## making gender a str so its caught by pipeline\n",
    "df['gender'] = df['gender'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split & Final Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:12.773542Z",
     "start_time": "2021-05-14T21:12:12.762261Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying root names of types of features to loop through and filter out from df\n",
    "target_col = 'class'\n",
    "drop_cols = ['id']\n",
    "\n",
    "y = df[target_col].copy()\n",
    "X = df.drop(columns=[target_col,*drop_cols]).copy()\n",
    "y.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:13.418679Z",
     "start_time": "2021-05-14T21:12:12.775280Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:13.422673Z",
     "start_time": "2021-05-14T21:12:13.420407Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:13.436865Z",
     "start_time": "2021-05-14T21:12:13.424346Z"
    }
   },
   "outputs": [],
   "source": [
    "## saving list of numeric vs categorical feature\n",
    "num_cols = list(X_train.select_dtypes('number').columns)\n",
    "cat_cols = list(X_train.select_dtypes('object').columns)\n",
    "\n",
    "## create pipelines and column transformer\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    ('scale',MinMaxScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='constant',fill_value='MISSING')),\n",
    "    ('encoder',OneHotEncoder(sparse=False,drop='first'))])\n",
    "\n",
    "print('# of num_cols:',len(num_cols))\n",
    "print('# of cat_cols:',len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:13.490667Z",
     "start_time": "2021-05-14T21:12:13.438462Z"
    }
   },
   "outputs": [],
   "source": [
    "## COMBINE BOTH PIPELINES INTO ONE WITH COLUMN TRANSFORMER\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('num',num_transformer,num_cols),\n",
    "    ('cat',cat_transformer,cat_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:13.557513Z",
     "start_time": "2021-05-14T21:12:13.492956Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fit preprocessing pipeline on training data and pull out the feature names and X_cols\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Use the encoder's .get_feature_names\n",
    "cat_features = list(preprocessor.named_transformers_['cat'].named_steps['encoder']\\\n",
    "                            .get_feature_names(cat_cols))\n",
    "X_cols = num_cols+cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:14.230534Z",
     "start_time": "2021-05-14T21:12:13.559220Z"
    }
   },
   "outputs": [],
   "source": [
    "## Transform X_traian,X_test and remake dfs\n",
    "X_train_df = pd.DataFrame(preprocessor.transform(X_train),\n",
    "                          index=X_train.index, columns=X_cols)\n",
    "X_test_df = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                          index=X_test.index, columns=X_cols)\n",
    "\n",
    "## Tranform X_train and X_test and make into DataFrames\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling with SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:14.243442Z",
     "start_time": "2021-05-14T21:12:14.236862Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:14.248491Z",
     "start_time": "2021-05-14T21:12:14.245619Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save list of trues and falses for each cols\n",
    "smote_feats = [False]*len(num_cols) +[True]*len(cat_features)\n",
    "# smote_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T21:12:14.596924Z",
     "start_time": "2021-05-14T21:12:14.250323Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## resample training data\n",
    "smote = SMOTENC(smote_feats)\n",
    "X_train_sm,y_train_sm = smote.fit_resample(X_train_df,y_train)\n",
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:46:35.251167Z",
     "start_time": "2021-05-14T19:46:33.796557Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tic = time() #timing!\n",
    "\n",
    "svc_linear = SVC(kernel='linear', probability=True,C=1)\n",
    "svc_linear.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "y_pred_train = svc_linear.predict(X_train_df)\n",
    "y_pred_test = svc_linear.predict(X_test_df)\n",
    "\n",
    "toc = time()\n",
    "print(f\"Run time is {toc-tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T19:46:35.492975Z",
     "start_time": "2021-05-14T19:46:35.253526Z"
    }
   },
   "outputs": [],
   "source": [
    "# how'd we do?\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix , accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred_test)) \n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "plot_confusion_matrix(svc_linear, X_test_df, y_test,cmap='Blues',normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iNTERPRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS & RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Summarize your conclusions and bullet-point your list of recommendations, which are based on your modeling results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
